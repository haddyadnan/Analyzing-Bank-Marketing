{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "modelling.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_mxoA7GwG0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTRSkEowG07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "813fbb75-45ab-4c31-90e0-23715906c777"
      },
      "source": [
        "data = pd.read_csv(\"bank-additional-full.csv\", sep = ';') ; data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>emp.var.rate</th>\n",
              "      <th>cons.price.idx</th>\n",
              "      <th>cons.conf.idx</th>\n",
              "      <th>euribor3m</th>\n",
              "      <th>nr.employed</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56</td>\n",
              "      <td>housemaid</td>\n",
              "      <td>married</td>\n",
              "      <td>basic.4y</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>high.school</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>149</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>high.school</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>226</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "      <td>admin.</td>\n",
              "      <td>married</td>\n",
              "      <td>basic.6y</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>high.school</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>307</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age        job  marital  ... euribor3m nr.employed   y\n",
              "0   56  housemaid  married  ...     4.857      5191.0  no\n",
              "1   57   services  married  ...     4.857      5191.0  no\n",
              "2   37   services  married  ...     4.857      5191.0  no\n",
              "3   40     admin.  married  ...     4.857      5191.0  no\n",
              "4   56   services  married  ...     4.857      5191.0  no\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe1d3gUCwG1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddf5b244-fa83-49ad-c30a-bf85e3ed9193"
      },
      "source": [
        "#%%writefile data.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, QuantileTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def preprocess(df):#, linear = False):\n",
        "\n",
        "    '''\n",
        "    parameters\n",
        "    ----------\n",
        "    df : A pandas dataframe\n",
        "          should contain the data\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    X_train, X_test, y_train, y_test\n",
        "      A pandas dataframe of the train and test set that has been transformed with PCA and minority class over sampling\n",
        "    \n",
        "    '''\n",
        "    #perform feature engineering and data preprocessing\n",
        "    d = {'yes':1, 'no':0}\n",
        "    y = df.y.replace(d)\n",
        "    df['new_pdays'] = df['pdays'].apply(lambda x: 1 if x < 16 else 0 if x > 30 else 2)\n",
        "    df['new_pdays'] = df['previous'] * df['new_pdays']\n",
        "    df['new_emp_rate'] = df['emp.var.rate'].apply(lambda x: 0 if x > 0 else 1)\n",
        "    df['empxemp'] = (df['nr.employed'] / df['euribor3m'])# + df[emp.var.rate]\n",
        "    \n",
        "    #Turn categorical columns to dummies\n",
        "    X = pd.get_dummies(df.drop(['y','cons.conf.idx','previous'], axis = 1))\n",
        "    \n",
        "    #Split data into train and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .1, random_state = 12)\n",
        "    print('The shape of the train set is {}, the shape of the test set is {}'.format(X_train.shape, X_test.shape))\n",
        "    \n",
        "   # if linear:\n",
        "#         print('Generating  polynomials')\n",
        "\n",
        "#         poly = PolynomialFeatures(2)\n",
        "#         X_train = poly.fit_transform(X_train)\n",
        "#         X_test = poly.transform(X_test)\n",
        "        \n",
        "    #Scale the data\n",
        "    S = StandardScaler()\n",
        "    \n",
        "    #Apply the transformation on the train and test set \n",
        "    X_train = S.fit_transform(X_train)\n",
        "    X_test = S.transform(X_test)\n",
        "    \n",
        "    #Apply PCA on the train and test set\n",
        "    pca = PCA()\n",
        "    x_train_pca = pca.fit_transform(X_train)\n",
        "    x_test_pca = pca.transform(X_test)\n",
        "    \n",
        "    #Oversample the minority class\n",
        "    sm = SMOTE(k_neighbors=10, random_state= 10)\n",
        "    \n",
        "    X_m, y_m = sm.fit_sample(x_train_pca, y_train)\n",
        "    \n",
        "    return X_m, x_test_pca, y_m, y_test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing data.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoJ_0gk5wG1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6a026e49-0d54-480c-956d-ba7508a4b27d"
      },
      "source": [
        "X_train, X_test, y_train, y_test = preprocess(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the train set is (37069, 64), the shape of the test set is (4119, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoPfzbnxHnYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtwM0pYWHyTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe63d998-94e9-4264-faef-b0d28bf66caa"
      },
      "source": [
        "%%writefile model.py\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold, KFold\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from hyperopt import hp, tpe\n",
        "from hyperopt.fmin import fmin\n",
        "import numpy as np\n",
        "\n",
        "class model():\n",
        "\n",
        "  def __init__(self, X_train, X_test, y_train, y_test):\n",
        "\n",
        "    '''\n",
        "    This class implements Logistic Regression, Multi layer Perceptron and extreme gradient boosting algorithm\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : Dataframe, numpy 2D array\n",
        "               The train set to be used for training\n",
        "    X_test :  Dataframe, Numpy 2D array\n",
        "              The hold out set, to be used for validating the model perfomancce\n",
        "    y_train : pandas series, numpy 1D array\n",
        "               Labels for the train set\n",
        "    y_test : pandas series, numpy 1D array\n",
        "              Label for the test set\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    logit : To fit the data using logistic regression\n",
        "    MLP : To fit the data using Multi layered perceptron\n",
        "    XGB : To fit the data using extreme gradient boosting\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Score\n",
        "      A 5 fold cross validation score\n",
        "    \n",
        "    Example\n",
        "    ------\n",
        "    M = model(X_train, X_test, y_train, y_test)\n",
        "    M.logit() #To fit logistic regression\n",
        "    '''\n",
        "\n",
        "    self.X_train = X_train\n",
        "    self.X_test = X_test\n",
        "    self.y_train = y_train\n",
        "    self.y_test = y_test\n",
        "\n",
        "\n",
        "  def evaluate(self, X_train, X_test, y_train, y_test, model):\n",
        "\n",
        "    '''\n",
        "    Evaluate the performance of the model\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    X_train : Dataframe, numpy 2D array\n",
        "               The train set to be used for training\n",
        "    X_test :  Dataframe, Numpy 2D array\n",
        "              The hold out set, to be used for validating the model perfomancce\n",
        "    y_train : pandas series, numpy 1D array\n",
        "               Labels for the train set\n",
        "    y_test : pandas series, numpy 1D array\n",
        "              Label for the test set\n",
        "    model : instance\n",
        "            A fitted instance of the model\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    f1_score on the train set and test set\n",
        "    classification report of the test set\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    evaluate(X_train, X_test, y_train, y_test)\n",
        "    \n",
        "\n",
        "    '''\n",
        "\n",
        "    #Obtain train and test f1 score\n",
        "    train_score = f1_score(y_train, model.predict(X_train))\n",
        "    test_score =  f1_score(y_test, model.predict(X_test))\n",
        "\n",
        "    #Print scores and classification report\n",
        "    print (f'train score is {train_score}, test_score is {test_score}')\n",
        "    print('----------------------------------------------------------')\n",
        "    print(classification_report(y_test, model.predict(X_test)))\n",
        "\n",
        "  \n",
        "  def objective_xgb(self, params):\n",
        "\n",
        "    '''\n",
        "    Define optimization objective for XGBOOST\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    params : dict\n",
        "            Model parameters to be optimized\n",
        "\n",
        "    Return:\n",
        "    ------\n",
        "    Cross validationn score\n",
        "\n",
        "    '''\n",
        "\n",
        "    #set parameters to tune\n",
        "    params = {\n",
        "        'max_depth': int(params['max_depth']),\n",
        "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
        "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
        "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
        "    }\n",
        "    \n",
        "    #fit model with parameters\n",
        "    xgb = XGBClassifier(random_state=23, **params, n_estimators=300)\n",
        "    #get cv score\n",
        "    skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    #Obtain cross validation score\n",
        "    score_skf = cross_val_score(xgb, self.X_train, self.y_train, scoring='f1', cv = skf).mean()\n",
        "    score_kf = cross_val_score(xgb, self.X_train, self.y_train, scoring='f1', cv = kf).mean()\n",
        "\n",
        "    #print scores\n",
        "    print(\"stratifiedKFold score {}, Kfold_score {}, params {}\".format(score_skf,score_kf, params))\n",
        "    return score_skf\n",
        "\n",
        "\n",
        "  def logit(self):\n",
        "    '''\n",
        "    Fit Logistic regression model\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Cross validation score\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    #Call the model\n",
        "    print('fitting Logistic regression...')\n",
        "    lr = LogisticRegression(random_state= 10, max_iter = 10000, )\n",
        "    #fit model\n",
        "    lr.fit(self.X_train, self.y_train)\n",
        "    #Obtain and print AUC Score for test and train\n",
        "    self.evaluate(self.X_train, self.X_test, self.y_train, self.y_test, lr)\n",
        "    \n",
        "    '''Hyper Parameter Search and Cross Validation for logistic Regression'''\n",
        "    \n",
        "    print('Searching for best hyperparameter... ')\n",
        "    #Set params\n",
        "    params = {'C': np.linspace(0.0001,0.001,20)}\n",
        "    \n",
        "    #Init and fit grid search\n",
        "    lr_grid = RandomizedSearchCV(LogisticRegression(random_state = 10, max_iter = 10000), params,\n",
        "                                 scoring='f1', cv =10,  n_iter = 20)\n",
        "    lr_grid.fit(self.X_train, self.y_train)\n",
        "    print('--------------------------------------\\nDONE')\n",
        "    print(f'Best Score {lr_grid.best_score_} Best Param {lr_grid.best_params_}')\n",
        "    self.evaluate(self.X_train, self.X_test, self.y_train, self.y_test,lr_grid.best_estimator_ )\n",
        "    \n",
        "    print('Running Cross Val \\nReturning 5fold CV Scores')\n",
        "    print('--------------------------------------')\n",
        "    \n",
        "    #stratified kfold\n",
        "    skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    skf_score = cross_val_score(lr_grid.best_estimator_, self.X_train, self.y_train, scoring = 'f1', cv = skf).mean()\n",
        "    kf_score = cross_val_score(lr_grid.best_estimator_, self.X_train, self.y_train, scoring = 'f1', cv = kf).mean()\n",
        "\n",
        "    \n",
        "    print('--------------------------------------\\nDONE')\n",
        "    print('StratifiedKfold Score: {}, KFold Score: {}'.format(skf_score, kf_score))\n",
        "  \n",
        "\n",
        "  def MLP(self):\n",
        "    '''\n",
        "    Fit MLP model\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    Cross validation score\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    #Call the model\n",
        "    print('fitting MLP...')\n",
        "    mlp = MLPClassifier(random_state= 10, early_stopping= True, learning_rate= 'adaptive')\n",
        "    #fit model\n",
        "    mlp.fit(self.X_train, self.y_train)\n",
        "    #Obtain and print AUC Score for test and train\n",
        "    self.evaluate(self.X_train, self.X_test, self.y_train, self.y_test, mlp)\n",
        "    \n",
        "    '''Hyper Parameter Search and Cross Validation for logistic Regression'''\n",
        "    \n",
        "    print('Searching for best hyperparameter... ')\n",
        "    #Set params\n",
        "    params = {'hidden_layer_sizes': np.arange(100,600,100), \n",
        "             'learning_rate_init': np.linspace(0.001,0.003,5)}\n",
        "    \n",
        "    #Init and fit grid search\n",
        "    mlp_grid = RandomizedSearchCV(MLPClassifier(random_state= 10, early_stopping= True, learning_rate= 'adaptive'), params,\n",
        "                                 scoring='f1', cv =10,  n_iter = 20, n_jobs = 12)\n",
        "    mlp_grid.fit(self.X_train, self.y_train)\n",
        "    print('--------------------------------------\\nDONE')\n",
        "    print(f'Best Score {mlp_grid.best_score_} Best Param {mlp_grid.best_params_}')\n",
        "    self.evaluate(self._train, self.X_test, self.y_train, self.y_test,mlp_grid.best_estimator_ )\n",
        "    \n",
        "    print('Running Cross Val \\nReturning 5fold CV Scores')\n",
        "    print('--------------------------------------')\n",
        "    \n",
        "    #stratified kfold\n",
        "    skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    skf_score = cross_val_score(mlp_grid.best_estimator_, self.X_train, self.y_train, scoring = 'f1', cv = skf).mean()\n",
        "    kf_score = cross_val_score(mlp_grid.best_estimator_, self.X_train, self.y_train, scoring = 'f1', cv = kf).mean()\n",
        "\n",
        "    \n",
        "    print('--------------------------------------\\nDONE')\n",
        "    print('StratifiedKfold Score: {}, KFold Score: {}'.format(skf_score, kf_score))\n",
        "\n",
        "\n",
        "  def XGB(self):\n",
        "    '''\n",
        "    Fit XGBOOST\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    Cross validation score\n",
        "    '''\n",
        "    \n",
        "    #Call the model\n",
        "    print('fitting xgboost...')\n",
        "    xgb = XGBClassifier(random_state= 10, n_estimators = 1000,  use_best_model = True, verbosity=0)\n",
        "    #fit model\n",
        "    xgb.fit(self.X_train, self.y_train, eval_set = [(self.X_test, self.y_test)], eval_metric = 'auc',\n",
        "            early_stopping_rounds = 100, verbose = 0 )\n",
        "    #Obtain and print AUC Score for test and train\n",
        "    self.evaluate(self.X_train, self.X_test, self.y_train, self.y_test, xgb)\n",
        "    \n",
        "    '''Hyper Parameter Search and Cross Validation for logistic Regression'''\n",
        "    \n",
        "    print('Searching for best hyperparameter... ')\n",
        "    #Set params\n",
        "    \n",
        "    space_xgb = {\n",
        "        'max_depth': hp.quniform('max_depth', 2, 5, 1),\n",
        "        'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
        "        'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
        "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.5)\n",
        "    }\n",
        "\n",
        "    print('Running Cross Val \\nReturning 5fold CV Scores')\n",
        "    print('--------------------------------------')\n",
        "    \n",
        "    best_xgb = fmin(fn=self.objective_xgb,\n",
        "            space=space_xgb,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=10)\n",
        "    \n",
        "    best_xgb['max_depth'] = int(best_xgb['max_depth'])\n",
        "    \n",
        "    print('--------------------------------------\\nDONE')\n",
        "    print(f'Best Params {best_xgb}')\n",
        "    \n",
        "    skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    xgb = XGBClassifier(n_estimators=10, random_state = 42, **best_xgb)\n",
        "\n",
        "    skf_score = cross_val_score(xgb, self.X_train, self.y_train, scoring = 'f1', cv = skf).mean()\n",
        "    kf_score = cross_val_score(xgb, self.X_train, self.y_train, scoring = 'f1', cv = kf).mean()\n",
        "    \n",
        "    \n",
        "    print('--------------------------------------\\nDONE')\n",
        "    print('StratifiedKfold Score: {}, KFold Score: {}'.format(skf_score, kf_score))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hQoIDiZ41Ia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52780631-c37d-4f41-95b2-3ea920aa811a"
      },
      "source": [
        "m = model(*preprocess(data))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the train set is (37069, 64), the shape of the test set is (4119, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0uKzNezUZDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "00a52121-55fc-4bb5-953d-b9482ce8d0b9"
      },
      "source": [
        "m.logit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fitting Logistic regression...\n",
            "train score is 0.7204054431046774, test_score is 0.36596736596736595\n",
            "----------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.74      0.83      3666\n",
            "           1       0.25      0.69      0.37       453\n",
            "\n",
            "    accuracy                           0.74      4119\n",
            "   macro avg       0.60      0.72      0.60      4119\n",
            "weighted avg       0.87      0.74      0.78      4119\n",
            "\n",
            "Searching for best hyperparameter... \n",
            "--------------------------------------\n",
            "DONE\n",
            "Best Score 0.7207422962145781 Best Param {'C': 0.0001}\n",
            "train score is 0.7205527831094051, test_score is 0.3648725212464589\n",
            "----------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.73      0.83      3666\n",
            "           1       0.25      0.71      0.36       453\n",
            "\n",
            "    accuracy                           0.73      4119\n",
            "   macro avg       0.60      0.72      0.60      4119\n",
            "weighted avg       0.88      0.73      0.78      4119\n",
            "\n",
            "Running Cross Val \n",
            "Returning 5fold CV Scores\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "DONE\n",
            "StratifiedKfold Score: 0.7210462079847259, KFold Score: 0.7209996690135231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCk95t2oUclT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ee4dcf66-3a09-40cc-9c80-d84d00fe6e95"
      },
      "source": [
        "m.MLP()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fitting MLP...\n",
            "train score is 0.8013555389335818, test_score is 0.41269841269841273\n",
            "----------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.81      0.87      3666\n",
            "           1       0.30      0.66      0.41       453\n",
            "\n",
            "    accuracy                           0.79      4119\n",
            "   macro avg       0.63      0.73      0.64      4119\n",
            "weighted avg       0.88      0.79      0.82      4119\n",
            "\n",
            "Searching for best hyperparameter... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SizS1oOBUoqK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "fc65136c-8f96-4c2b-88cc-01ca19eccac6"
      },
      "source": [
        "m.XGB()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fitting xgboost...\n",
            "train score is 0.8444356169406588, test_score is 0.4623493975903615\n",
            "----------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.85      0.90      3666\n",
            "           1       0.35      0.68      0.46       453\n",
            "\n",
            "    accuracy                           0.83      4119\n",
            "   macro avg       0.65      0.76      0.68      4119\n",
            "weighted avg       0.89      0.83      0.85      4119\n",
            "\n",
            "Searching for best hyperparameter... \n",
            "Running Cross Val \n",
            "Returning 5fold CV Scores\n",
            "--------------------------------------\n",
            "stratifiedKFold score 0.8609257031402097, Kfold_score 0.8605168737209329, params {'max_depth': 4, 'gamma': '0.294', 'learning_rate': '0.422', 'colsample_bytree': '0.987'}\n",
            "stratifiedKFold score 0.8711761593722912, Kfold_score 0.8700573148587271, params {'max_depth': 5, 'gamma': '0.047', 'learning_rate': '0.422', 'colsample_bytree': '0.527'}\n",
            "stratifiedKFold score 0.7689762664961968, Kfold_score 0.7696221366741784, params {'max_depth': 3, 'gamma': '0.110', 'learning_rate': '0.097', 'colsample_bytree': '0.885'}\n",
            "stratifiedKFold score 0.8613101244463197, Kfold_score 0.8618175310084428, params {'max_depth': 4, 'gamma': '0.005', 'learning_rate': '0.495', 'colsample_bytree': '0.954'}\n",
            "stratifiedKFold score 0.775395033556822, Kfold_score 0.7750864610581354, params {'max_depth': 3, 'gamma': '0.224', 'learning_rate': '0.117', 'colsample_bytree': '0.628'}\n",
            "stratifiedKFold score 0.796843066513577, Kfold_score 0.796234162968038, params {'max_depth': 3, 'gamma': '0.080', 'learning_rate': '0.165', 'colsample_bytree': '0.993'}\n",
            "stratifiedKFold score 0.857951612784615, Kfold_score 0.8584266631347625, params {'max_depth': 4, 'gamma': '0.151', 'learning_rate': '0.499', 'colsample_bytree': '0.689'}\n",
            "stratifiedKFold score 0.8041400913312573, Kfold_score 0.8046621836391834, params {'max_depth': 3, 'gamma': '0.349', 'learning_rate': '0.221', 'colsample_bytree': '0.574'}\n",
            "stratifiedKFold score 0.8608004895328237, Kfold_score 0.8620249897079869, params {'max_depth': 4, 'gamma': '0.200', 'learning_rate': '0.478', 'colsample_bytree': '0.994'}\n",
            "stratifiedKFold score 0.84483137701772, Kfold_score 0.8464201408318619, params {'max_depth': 4, 'gamma': '0.206', 'learning_rate': '0.246', 'colsample_bytree': '0.834'}\n",
            "100%|██████████| 10/10 [23:16<00:00, 139.65s/it, best loss: 0.7689762664961968]\n",
            "--------------------------------------\n",
            "DONE\n",
            "Best Params {'colsample_bytree': 0.8849415668975888, 'gamma': 0.11032749933929553, 'learning_rate': 0.09686095008373695, 'max_depth': 3}\n",
            "--------------------------------------\n",
            "DONE\n",
            "StratifiedKfold Score: 0.7160902352106682, KFold Score: 0.7173271469992759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nomxNc5wp5OI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d498097-8fcf-490d-870d-aa0cd8da9076"
      },
      "source": [
        "%%writefile main.py\n",
        "from data import preprocess\n",
        "from model import model\n",
        "def main(data):\n",
        "\n",
        "  '''\n",
        "  The Main function to automate the processess\n",
        "\n",
        "  '''\n",
        "\n",
        "  M = model(*preprocess(data))\n",
        "  M.logit()\n",
        "  M.XGB()\n",
        "  M.MLP()\n",
        "\n",
        "  if __name__ == 'main':\n",
        "    main()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSzEqLSfbqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx0q0b3WhGl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}